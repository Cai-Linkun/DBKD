{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import operator\n",
    "import os\n",
    "from os.path import expanduser\n",
    "import wget\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim.models\n",
    "import gensim.models.word2vec as w2v\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "MIMIC_3_DIR = \"./ori_mimic3_data/mimic3\"\n",
    "DATA_DIR = \"./ori_mimic3_data\"\n",
    "SAVE_DIR = \"./processed_data\"\n",
    "PAD_CHAR = \"**PAD**\"\n",
    "EMBEDDING_SIZE = 100\n",
    "MAX_LENGTH = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = 'full' #use all available labels in the dataset for prediction\n",
    "notes_file = '%s/NOTEEVENTS.csv' % MIMIC_3_DIR # raw note events downloaded from MIMIC-III\n",
    "vocab_size = 'full' #don't limit the vocab size to a specific number\n",
    "vocab_min = 3 #discard tokens appearing in fewer than this many documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfproc = pd.read_csv('%s/PROCEDURES_ICD.csv' % MIMIC_3_DIR)\n",
    "dfdiag = pd.read_csv('%s/DIAGNOSES_ICD.csv' % MIMIC_3_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat(code, is_diag):\n",
    "    code = ''.join(code.split('.'))\n",
    "    if is_diag:\n",
    "        if code.startswith('E'):\n",
    "            if len(code) > 4:\n",
    "                code = code[:4] + '.' + code[4:]\n",
    "        else:\n",
    "            if len(code) > 3:\n",
    "                code = code[:3] + '.' + code[3:]\n",
    "    else:\n",
    "        code = code[:2] + '.' + code[2:]\n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdiag['code'] = dfdiag.apply(lambda row: str(reformat(str(row[4]), True)), axis=1)\n",
    "dfproc['code'] = dfproc.apply(lambda row: str(reformat(str(row[4]), False)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcodes = pd.concat([dfdiag, dfproc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcodes.to_csv('%s/ALL_CODES.csv' % SAVE_DIR, index=False,\n",
    "               columns=['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'SEQ_NUM', 'code'],\n",
    "               header=['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'SEQ_NUM', 'ICD9_CODE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the full dataset (not just discharge summaries)\n",
    "df = pd.read_csv('%s/ALL_CODES.csv' % SAVE_DIR, dtype={\"ICD9_CODE\": str})\n",
    "len(df['ICD9_CODE'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def write_discharge_summaries(out_file):\n",
    "    notes_file = '%s/NOTEEVENTS.csv' % (MIMIC_3_DIR)\n",
    "    print(\"processing notes file\")\n",
    "    with open(notes_file, 'r') as csvfile:\n",
    "        with open(out_file, 'w') as outfile:\n",
    "            print(\"writing to %s\" % (out_file))\n",
    "            outfile.write(','.join(['SUBJECT_ID', 'HADM_ID', 'CHARTTIME', 'TEXT']) + '\\n')\n",
    "            notereader = csv.reader(csvfile)\n",
    "            #header\n",
    "            next(notereader)\n",
    "            i = 0\n",
    "            for line in tqdm(notereader):\n",
    "                subj = int(line[1])\n",
    "                category = line[6]\n",
    "                if category == \"Discharge summary\":\n",
    "                    note = line[10]\n",
    "                    #tokenize, lowercase and remove numerics\n",
    "                    tokens = [t.lower() for t in tokenizer.tokenize(note) if not t.isnumeric()]\n",
    "                    text = '\"' + ' '.join(tokens) + '\"'\n",
    "                    outfile.write(','.join([line[1], line[2], line[4], text]) + '\\n')\n",
    "                i += 1\n",
    "    return out_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disch_full_file = write_discharge_summaries(out_file=\"%s/disch_full.csv\" % SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('%s/disch_full.csv' % SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['HADM_ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokens and types\n",
    "types = set()\n",
    "num_tok = 0\n",
    "for row in df.itertuples():\n",
    "    for w in row[4].split():\n",
    "        types.add(w)\n",
    "        num_tok += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num types\", len(types))\n",
    "print(\"Num tokens\", str(num_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's sort by SUBJECT_ID and HADM_ID to make a correspondence with the MIMIC-3 label file\n",
    "df = df.sort_values(['SUBJECT_ID', 'HADM_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort the label file by the same\n",
    "dfl = pd.read_csv('%s/ALL_CODES.csv' % SAVE_DIR)\n",
    "dfl = dfl.sort_values(['SUBJECT_ID', 'HADM_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['HADM_ID'].unique()), len(dfl['HADM_ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's filter out these HADM_ID's\n",
    "hadm_ids = set(df['HADM_ID'])\n",
    "with open('%s/ALL_CODES.csv' % SAVE_DIR, 'r') as lf:\n",
    "    with open('%s/ALL_CODES_filtered.csv' % SAVE_DIR, 'w') as of:\n",
    "        w = csv.writer(of)\n",
    "        w.writerow(['SUBJECT_ID', 'HADM_ID', 'ICD9_CODE', 'ADMITTIME', 'DISCHTIME'])\n",
    "        r = csv.reader(lf)\n",
    "        #header\n",
    "        next(r)\n",
    "        for i,row in enumerate(r):\n",
    "            hadm_id = int(row[2])\n",
    "            #print(hadm_id)\n",
    "            #break\n",
    "            if hadm_id in hadm_ids:\n",
    "                w.writerow(row[1:3] + [row[-1], '', ''])\n",
    "dfl = pd.read_csv('%s/ALL_CODES_filtered.csv' % SAVE_DIR, index_col=None)\n",
    "len(dfl['HADM_ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATETIME_FORMAT = \"%Y-%m-%d %H:%M:%S\"\n",
    "\n",
    "def concat_data(labelsfile, notes_file):\n",
    "    with open(labelsfile, 'r') as lf:\n",
    "        print(\"CONCATENATING\")\n",
    "        with open(notes_file, 'r') as notesfile:\n",
    "            outfilename = '%s/notes_labeled.csv' % SAVE_DIR\n",
    "            with open(outfilename, 'w') as outfile:\n",
    "                w = csv.writer(outfile)\n",
    "                w.writerow(['SUBJECT_ID', 'HADM_ID', 'TEXT', 'LABELS'])\n",
    "\n",
    "                labels_gen = next_labels(lf)\n",
    "                notes_gen = next_notes(notesfile)\n",
    "\n",
    "                for i, (subj_id, text, hadm_id) in enumerate(notes_gen):\n",
    "                    if i % 10000 == 0:\n",
    "                        print(str(i) + \" done\")\n",
    "                    cur_subj, cur_labels, cur_hadm = next(labels_gen)\n",
    "\n",
    "                    if cur_hadm == hadm_id:\n",
    "                        w.writerow([subj_id, str(hadm_id), text, ';'.join(cur_labels)])\n",
    "                    else:\n",
    "                        print(\"couldn't find matching hadm_id. data is probably not sorted correctly\")\n",
    "                        break\n",
    "                    \n",
    "    return outfilename\n",
    "\n",
    "def split_data(labeledfile, base_name):\n",
    "    print(\"SPLITTING\")\n",
    "    #create and write headers for train, dev, test\n",
    "    train_name = '%s_train_split.csv' % (base_name)\n",
    "    dev_name = '%s_dev_split.csv' % (base_name)\n",
    "    test_name = '%s_test_split.csv' % (base_name)\n",
    "    train_file = open(train_name, 'w')\n",
    "    dev_file = open(dev_name, 'w')\n",
    "    test_file = open(test_name, 'w')\n",
    "    train_file.write(','.join(['SUBJECT_ID', 'HADM_ID', 'TEXT', 'LABELS']) + \"\\n\")\n",
    "    dev_file.write(','.join(['SUBJECT_ID', 'HADM_ID', 'TEXT', 'LABELS']) + \"\\n\")\n",
    "    test_file.write(','.join(['SUBJECT_ID', 'HADM_ID', 'TEXT', 'LABELS']) + \"\\n\")\n",
    "\n",
    "    hadm_ids = {}\n",
    "\n",
    "    #read in train, dev, test splits\n",
    "    for splt in ['train', 'dev', 'test']:\n",
    "        hadm_ids[splt] = set()\n",
    "        with open('%s/%s_full_hadm_ids.csv' % (MIMIC_3_DIR, splt), 'r') as f:\n",
    "            for line in f:\n",
    "                hadm_ids[splt].add(line.rstrip())\n",
    "\n",
    "    with open(labeledfile, 'r') as lf:\n",
    "        reader = csv.reader(lf)\n",
    "        next(reader)\n",
    "        i = 0\n",
    "        cur_hadm = 0\n",
    "        for row in reader:\n",
    "            #filter text, write to file according to train/dev/test split\n",
    "            if i % 10000 == 0:\n",
    "                print(str(i) + \" read\")\n",
    "\n",
    "            hadm_id = row[1]\n",
    "\n",
    "            if hadm_id in hadm_ids['train']:\n",
    "                train_file.write(','.join(row) + \"\\n\")\n",
    "            elif hadm_id in hadm_ids['dev']:\n",
    "                dev_file.write(','.join(row) + \"\\n\")\n",
    "            elif hadm_id in hadm_ids['test']:\n",
    "                test_file.write(','.join(row) + \"\\n\")\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        train_file.close()\n",
    "        dev_file.close()\n",
    "        test_file.close()\n",
    "    return train_name, dev_name, test_name\n",
    "    \n",
    "def next_labels(labelsfile):\n",
    "    labels_reader = csv.reader(labelsfile)\n",
    "    #header\n",
    "    next(labels_reader)\n",
    "\n",
    "    first_label_line = next(labels_reader)\n",
    "\n",
    "    cur_subj = int(first_label_line[0])\n",
    "    cur_hadm = int(first_label_line[1])\n",
    "    cur_labels = [first_label_line[2]]\n",
    "\n",
    "    for row in labels_reader:\n",
    "        subj_id = int(row[0])\n",
    "        hadm_id = int(row[1])\n",
    "        code = row[2]\n",
    "        #keep reading until you hit a new hadm id\n",
    "        if hadm_id != cur_hadm or subj_id != cur_subj:\n",
    "            yield cur_subj, cur_labels, cur_hadm\n",
    "            cur_labels = [code]\n",
    "            cur_subj = subj_id\n",
    "            cur_hadm = hadm_id\n",
    "        else:\n",
    "            #add to the labels and move on\n",
    "            cur_labels.append(code)\n",
    "    yield cur_subj, cur_labels, cur_hadm\n",
    "\n",
    "def next_notes(notesfile):\n",
    "    nr = csv.reader(notesfile)\n",
    "    #header\n",
    "    next(nr)\n",
    "\n",
    "    first_note = next(nr)\n",
    "\n",
    "    cur_subj = int(first_note[0])\n",
    "    cur_hadm = int(first_note[1])\n",
    "    cur_text = first_note[3]\n",
    "    \n",
    "    for row in nr:\n",
    "        subj_id = int(row[0])\n",
    "        hadm_id = int(row[1])\n",
    "        text = row[3]\n",
    "        #keep reading until you hit a new hadm id\n",
    "        if hadm_id != cur_hadm or subj_id != cur_subj:\n",
    "            yield cur_subj, cur_text, cur_hadm\n",
    "            cur_text = text\n",
    "            cur_subj = subj_id\n",
    "            cur_hadm = hadm_id\n",
    "        else:\n",
    "            #concatenate to the discharge summary and move on\n",
    "            cur_text += \" \" + text\n",
    "    yield cur_subj, cur_text, cur_hadm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we still need to sort it by HADM_ID\n",
    "dfl = dfl.sort_values(['SUBJECT_ID', 'HADM_ID'])\n",
    "dfl.to_csv('%s/ALL_CODES_filtered.csv' % SAVE_DIR, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's append each instance with all of its codes\n",
    "#this is pretty non-trivial so let's use this script I wrote, which requires the notes to be written to file\n",
    "sorted_file = '%s/disch_full.csv' % SAVE_DIR\n",
    "df.to_csv(sorted_file, index=False)\n",
    "labeled = concat_data('%s/ALL_CODES_filtered.csv' % SAVE_DIR, sorted_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name of the file we just made\n",
    "print(labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnl = pd.read_csv(labeled)\n",
    "#Tokens and types\n",
    "types = set()\n",
    "num_tok = 0\n",
    "for row in dfnl.itertuples():\n",
    "    for w in row[3].split():\n",
    "        types.add(w)\n",
    "        num_tok += 1\n",
    "print(\"num types\", len(types), \"num tokens\", num_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfnl['HADM_ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = '%s/notes_labeled.csv' % SAVE_DIR\n",
    "base_name = \"%s/disch\" % SAVE_DIR #for output\n",
    "tr, dv, te = split_data(fname, base_name=base_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(vocab_min, infile, vocab_filename):\n",
    "    with open(infile, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        #header\n",
    "        next(reader)\n",
    "\n",
    "        note_numwords = []\n",
    "        #indices where notes start\n",
    "        note_inds = [0]\n",
    "        #indices of discovered words\n",
    "        indices = []\n",
    "        #holds a bunch of ones\n",
    "        data = []\n",
    "        #keep track of discovered words\n",
    "        vocab = {}\n",
    "        #build lookup table for terms\n",
    "        num2term = {}\n",
    "        #preallocate array to hold number of notes each term appears in\n",
    "        note_occur = np.zeros(400000, dtype=int)\n",
    "        i = 0\n",
    "        for row in reader:\n",
    "            text = row[2]\n",
    "            numwords = 0\n",
    "            for term in text.split():\n",
    "                #put term in vocab if it's not there. else, get the index\n",
    "                index = vocab.setdefault(term, len(vocab))\n",
    "                indices.append(index)\n",
    "                num2term[index] = term\n",
    "                data.append(1)\n",
    "                numwords += 1\n",
    "            #record where the next note starts\n",
    "            note_inds.append(len(indices))\n",
    "            indset = set(indices[note_inds[-2]:note_inds[-1]])\n",
    "            #go thru all the word indices you just added, and add to the note occurrence count for each of them\n",
    "            for ind in indset:\n",
    "                note_occur[ind] += 1\n",
    "            note_numwords.append(numwords)\n",
    "            i += 1\n",
    "        #clip trailing zeros\n",
    "        note_occur = note_occur[note_occur>0]\n",
    "\n",
    "        #turn vocab into a list so indexing doesn't get fd up when we drop rows\n",
    "        vocab_list = np.array([word for word,ind in sorted(vocab.items(), key=operator.itemgetter(1))])\n",
    "\n",
    "        #1. create sparse document matrix\n",
    "        C = csr_matrix((data, indices, note_inds), dtype=int).transpose()\n",
    "        #also need the numwords array to be a sparse matrix\n",
    "        note_numwords = csr_matrix(1. / np.array(note_numwords))\n",
    "        \n",
    "        #2. remove rows with less than 3 total occurrences\n",
    "        #inds holds indices of rows corresponding to terms that occur in < 3 documents\n",
    "        inds = np.nonzero(note_occur >= vocab_min)[0]\n",
    "        print(str(len(inds)) + \" terms qualify out of \" + str(C.shape[0]) + \" total\")\n",
    "        #drop those rows\n",
    "        C = C[inds,:]\n",
    "        note_occur = note_occur[inds]\n",
    "        vocab_list = vocab_list[inds]\n",
    "\n",
    "        print(\"writing output\")\n",
    "        with open(vocab_filename, 'w') as vocab_file:\n",
    "            for word in vocab_list:\n",
    "                vocab_file.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_min = 3\n",
    "vname = '%s/vocab.csv' % SAVE_DIR\n",
    "build_vocab(vocab_min, tr, vname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for splt in ['train', 'dev', 'test']:\n",
    "    filename = '%s/disch_%s_split.csv' % (SAVE_DIR, splt)\n",
    "    df = pd.read_csv(filename)\n",
    "    df['length'] = df.apply(lambda row: len(str(row['TEXT']).split()), axis=1)\n",
    "    df = df.sort_values(['length'])\n",
    "    df.to_csv('%s/%s_full.csv' % (SAVE_DIR, splt), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessedIter(object):\n",
    "\n",
    "    def __init__(self, Y, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.filename) as f:\n",
    "            r = csv.reader(f)\n",
    "            next(r)\n",
    "            for row in r:\n",
    "                yield (row[3].split())\n",
    "\n",
    "def word_embeddings(Y, notes_file, embedding_size, min_count, epochs):\n",
    "    modelname = \"processed_%s.w2v\" % (Y)\n",
    "    sentences = ProcessedIter(Y, notes_file)\n",
    "\n",
    "    model = w2v.Word2Vec(vector_size=embedding_size, min_count=min_count, workers=4, sentences=sentences, epochs = epochs)\n",
    "    print(\"building word2vec vocab on %s...\" % (notes_file))\n",
    "    \n",
    "    model.build_vocab(sentences)\n",
    "    print(\"training...\")\n",
    "    model.train(sentences, total_examples=model.corpus_count, epochs=epochs)\n",
    "    out_file = '/'.join(notes_file.split('/')[:-1] + [modelname])\n",
    "    print(\"writing embeddings to %s\" % (out_file))\n",
    "    model.save(out_file)\n",
    "    return out_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_file = word_embeddings('full', '%s/disch_full.csv' % SAVE_DIR, 100, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gensim_to_embeddings(wv_file, vocab_file, Y, outfile=None):\n",
    "    model = gensim.models.Word2Vec.load(wv_file)\n",
    "    wv = model.wv\n",
    "    #free up memory\n",
    "    del model\n",
    "\n",
    "    vocab = set()\n",
    "    with open(vocab_file, 'r') as vocabfile:\n",
    "        for i,line in enumerate(vocabfile):\n",
    "            line = line.strip()\n",
    "            if line != '':\n",
    "                vocab.add(line)\n",
    "    ind2w = {i+1:w for i,w in enumerate(sorted(vocab))}\n",
    "\n",
    "    W, words = build_matrix(ind2w, wv)\n",
    "\n",
    "    if outfile is None:\n",
    "        outfile = wv_file.replace('.w2v', '.embed')\n",
    "\n",
    "    #smash that save button\n",
    "    save_embeddings(W, words, outfile)\n",
    "\n",
    "def build_matrix(ind2w, wv):\n",
    "    W = np.zeros((len(ind2w)+1, len(wv.word_vec(wv.index_to_key[0])) ))\n",
    "    words = [PAD_CHAR]\n",
    "    W[0][:] = np.zeros(len(wv.word_vec(wv.index_to_key[0])))\n",
    "    for idx, word in tqdm(ind2w.items()):\n",
    "        if idx >= W.shape[0]:\n",
    "            break    \n",
    "        W[idx][:] = wv.word_vec(word)\n",
    "        words.append(word)\n",
    "    return W, words\n",
    "\n",
    "def save_embeddings(W, words, outfile):\n",
    "    with open(outfile, 'w') as o:\n",
    "        #pad token already included\n",
    "        for i in range(len(words)):\n",
    "            line = [words[i]]\n",
    "            line.extend([str(d) for d in W[i]])\n",
    "            o.write(\" \".join(line) + \"\\n\")\n",
    "\n",
    "def load_embeddings(embed_file):\n",
    "    #also normalizes the embeddings\n",
    "    W = []\n",
    "    with open(embed_file) as ef:\n",
    "        for line in ef:\n",
    "            line = line.rstrip().split()\n",
    "            vec = np.array(line[1:]).astype(np.float)\n",
    "            vec = vec / float(np.linalg.norm(vec) + 1e-6)\n",
    "            W.append(vec)\n",
    "        #UNK embedding, gaussian randomly initialized \n",
    "        print(\"adding unk embedding\")\n",
    "        vec = np.random.randn(len(W[-1]))\n",
    "        vec = vec / float(np.linalg.norm(vec) + 1e-6)\n",
    "        W.append(vec)\n",
    "    W = np.array(W)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_to_embeddings('%s/processed_full.w2v' % SAVE_DIR, '%s/vocab.csv' % SAVE_DIR, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./ori_mimic3_data\"\n",
    "def load_code_descriptions():\n",
    "    desc_dict = defaultdict(str)\n",
    "    with open(\"%s/D_ICD_DIAGNOSES.csv\" % (DATA_DIR), 'r') as descfile:\n",
    "        r = csv.reader(descfile)\n",
    "        #header\n",
    "        next(r)\n",
    "        for row in r:\n",
    "            code = row[1]\n",
    "            desc = row[-1]\n",
    "            desc_dict[reformat(code, True)] = desc\n",
    "    with open(\"%s/D_ICD_PROCEDURES.csv\" % (DATA_DIR), 'r') as descfile:\n",
    "        r = csv.reader(descfile)\n",
    "        #header\n",
    "        next(r)\n",
    "        for row in r:\n",
    "            code = row[1]\n",
    "            desc = row[-1]\n",
    "            if code not in desc_dict.keys():\n",
    "                desc_dict[reformat(code, False)] = desc\n",
    "    with open('%s/ICD9_descriptions' % DATA_DIR, 'r') as labelfile:\n",
    "        for i,row in enumerate(labelfile):\n",
    "            row = row.rstrip().split()\n",
    "            code = row[0]\n",
    "            if code not in desc_dict.keys():\n",
    "                desc_dict[code] = ' '.join(row[1:])\n",
    "    return desc_dict\n",
    "\n",
    "def vocab_index_descriptions_ref(vocab_file, desc_file, vectors_file):\n",
    "    #load lookups\n",
    "    vocab = set()\n",
    "    with open(vocab_file, 'r') as vocabfile:\n",
    "        for i,line in enumerate(vocabfile):\n",
    "            line = line.strip()\n",
    "            if line != '':\n",
    "                vocab.add(line)\n",
    "    ind2w = {i+1:w for i,w in enumerate(sorted(vocab))}\n",
    "    w2ind = {w:i for i,w in ind2w.items()}\n",
    "    desc_dict = load_code_descriptions()\n",
    "        \n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    with open(vectors_file, 'w') as of:\n",
    "        with open(desc_file, 'w') as desc_f:\n",
    "            vw = csv.writer(of, delimiter=' ')\n",
    "            vw.writerow([\"CODE\", \"VECTOR\"])\n",
    "\n",
    "            dw = csv.writer(desc_f, delimiter=' ')\n",
    "            dw.writerow([\"CODE\", \"WORD\"])\n",
    "\n",
    "            for code, desc in tqdm(desc_dict.items()):\n",
    "                #same preprocessing steps as in get_discharge_summaries\n",
    "                tokens = [t.lower() for t in tokenizer.tokenize(desc) if not t.isnumeric()]\n",
    "                inds = [w2ind[t] if t in w2ind.keys() else len(w2ind)+1 for t in tokens]\n",
    "                vw.writerow([code] + [str(i) for i in inds])\n",
    "                dw.writerow([code] + [t for t in tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = \"./processed_data\"\n",
    "vocab_index_descriptions_ref('%s/vocab.csv' % SAVE_DIR,\n",
    "                             '%s/description_words.vocab' % SAVE_DIR,\n",
    "                             '%s/description_vectors.vocab' % SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import expanduser\n",
    "import requests\n",
    "import shutil\n",
    "import torch\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "SAVE_DIR = \"./processed_data\"\n",
    "BIO_BERT_LINK = \"https://www.dropbox.com/s/dc2ki2d4jv8isrb/biobert_weights.zip?dl=1\"\n",
    "MODEL_SAVE_DICT = \"./trained_models/\"\n",
    "TRAIN_DATA_PATH = os.path.join(SAVE_DIR, 'train_full.csv') # change this\n",
    "\n",
    "def download_and_extract(tgt_dir):\n",
    "    os.makedirs(tgt_dir, exist_ok=True)\n",
    "    response = requests.get(BIO_BERT_LINK, stream=True)\n",
    "    fp = os.path.join(tgt_dir, \"biobert_weights.zip\")\n",
    "    if response.status_code == 200:\n",
    "        with open(fp, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "        print(f\"download success: {fp}\")\n",
    "    else:\n",
    "        print(f\"download error: {response.status_code}\")\n",
    "    shutil.unpack_archive(fp, tgt_dir)\n",
    "    # os.remove(fp)\n",
    "\n",
    "def load_desc_map(desc_word_file):\n",
    "    c2desc_map = {}\n",
    "    with open(desc_word_file, 'r') as descfile:\n",
    "        r = csv.reader(descfile)\n",
    "        #header\n",
    "        next(r)\n",
    "        for row in r:\n",
    "            code = row[0]\n",
    "            desc = ' '.join(row[1:-1])\n",
    "            c2desc_map[code] = desc\n",
    "    return c2desc_map\n",
    "\n",
    "def buildc2idx(train_path):\n",
    "    codes = set()\n",
    "    for split in ['train', 'dev', 'test']:\n",
    "        with open(train_path.replace('train', split), 'r') as f:\n",
    "            lr = csv.reader(f)\n",
    "            next(lr)\n",
    "            for row in lr:\n",
    "                for code in row[3].split(';'):\n",
    "                    codes.add(code)\n",
    "    codes = set([c for c in codes if c != ''])\n",
    "    ind2c = defaultdict(str, {i:c for i,c in enumerate(sorted(codes))})\n",
    "    c2ind = {c:i for i,c in ind2c.items()}\n",
    "    return c2ind, ind2c\n",
    "\n",
    "def code_desc_biobert():\n",
    "    from transformers import BertConfig, BertModel, BertTokenizer\n",
    "    model_dir = os.path.join(MODEL_SAVE_DICT, \"biobert_v1.1_pubmed\")\n",
    "    assert os.path.exists(model_dir)\n",
    "\n",
    "    model = BertModel.from_pretrained(model_dir)\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "    # tokenizer = BertTokenizer(vocab_file=voc_dir, do_lower_case=False)\n",
    "    outputs = []\n",
    "\n",
    "    c2desc = load_desc_map(os.path.join(SAVE_DIR, 'description_words.vocab'))\n",
    "    c2ind, ind2c = buildc2idx(TRAIN_DATA_PATH)\n",
    "    idx = 1\n",
    "    for (cidx, code) in ind2c.items():\n",
    "        desc = c2desc[code]\n",
    "        inputs = tokenizer(desc, return_tensors='pt', padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            out = model(**inputs)\n",
    "            embedding = outputs.last_hidden_state\n",
    "            print(embedding.shape)\n",
    "        idx += 1\n",
    "        if idx > 2:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_and_extract(MODEL_SAVE_DICT)\n",
    "code_desc_biobert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = 50\n",
    "#first calculate the top k\n",
    "counts = Counter()\n",
    "dfnl = pd.read_csv('%s/notes_labeled.csv' % SAVE_DIR)\n",
    "for row in dfnl.itertuples():\n",
    "    for label in str(row[4]).split(';'):\n",
    "        counts[label] += 1\n",
    "codes_50 = sorted(counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "codes_50 = [code[0] for code in codes_50[:Y]]\n",
    "with open('%s/TOP_%s_CODES.csv' % (SAVE_DIR, str(Y)), 'w') as of:\n",
    "    w = csv.writer(of)\n",
    "    for code in codes_50:\n",
    "        w.writerow([code])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for splt in ['train', 'dev', 'test']:\n",
    "    print(splt)\n",
    "    hadm_ids = set()\n",
    "    with open('%s/%s_50_hadm_ids.csv' % (MIMIC_3_DIR, splt), 'r') as f:\n",
    "        for line in f:\n",
    "            hadm_ids.add(line.rstrip())\n",
    "    with open('%s/notes_labeled.csv' % SAVE_DIR, 'r') as f:\n",
    "        with open('%s/%s_%s.csv' % (SAVE_DIR, splt, str(Y)), 'w') as of:\n",
    "            r = csv.reader(f)\n",
    "            w = csv.writer(of)\n",
    "            #header\n",
    "            w.writerow(next(r))\n",
    "            i = 0\n",
    "            for row in r:\n",
    "                hadm_id = row[1]\n",
    "                if hadm_id not in hadm_ids:\n",
    "                    continue\n",
    "                codes = set(str(row[3]).split(';'))\n",
    "                filtered_codes = codes.intersection(set(codes_50))\n",
    "                if len(filtered_codes) > 0:\n",
    "                    w.writerow(row[:3] + [';'.join(filtered_codes)])\n",
    "                    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for splt in ['train', 'dev', 'test']:\n",
    "    filename = '%s/%s_%s.csv' % (SAVE_DIR, splt, str(Y))\n",
    "    df = pd.read_csv(filename)\n",
    "    df['length'] = df.apply(lambda row: len(str(row['TEXT']).split()), axis=1)\n",
    "    df = df.sort_values(['length'])\n",
    "    df.to_csv('%s/%s_%s.csv' % (SAVE_DIR, splt, str(Y)), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
